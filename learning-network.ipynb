{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mynet' from '/home/dan/Downloads/projects/neural_net/DeepLearningPython35/mynet.py'>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import network\n",
    "import numpy as np\n",
    "import copy\n",
    "import imp\n",
    "import mynet\n",
    "imp.reload(network)\n",
    "imp.reload(mynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4,2,3]\n",
    "net = network.Network(sizes)\n",
    "\n",
    "def printValues(listVals):\n",
    "   for i in range(len(listVals)):\n",
    "        print(listVals[i])\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original weights: (list of <class 'numpy.ndarray'>)\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.33021132 0.93187687 0.37859619 0.38582005]\n",
      " [0.23633977 0.64792759 0.08830015 0.25043338]]\n",
      "\n",
      "[[0.88792344 0.39566582]\n",
      " [0.17283531 0.01274666]\n",
      " [0.90396271 0.13828046]]\n",
      "\n",
      "shape of net.weights[0]: (2, 4)\n",
      "\n",
      "new weights:\n",
      "[[0.1 0.2 0.3 0.4]\n",
      " [1.1 1.2 1.3 1.4]]\n",
      "\n",
      "[[ 0.5 -0.5]\n",
      " [ 0.6 -1. ]\n",
      " [ 0.7 -2. ]]\n",
      "\n",
      "new biases: (list of <class 'numpy.ndarray'>)\n",
      "<class 'numpy.ndarray'>\n",
      "[[1.]\n",
      " [1.]]\n",
      "\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sizes = [4,2,3]\n",
    "net = network.Network(sizes)\n",
    "\n",
    "print(\"original weights: (list of \" + str(type(net.weights[0])) + \")\")\n",
    "print(type(net.weights[0]))\n",
    "printValues(net.weights)\n",
    "print(\"shape of net.weights[0]: \" + str(net.weights[0].shape))\n",
    "\n",
    "\n",
    "# manually set weights for connections between layers 0 and 1\n",
    "net.weights[0]=np.array([[0.1, 0.2, 0.3, 0.4], [1.1,1.2,1.3,1.4]])\n",
    "# same for layers 1 and 2\n",
    "net.weights[1]=np.array([[0.5, -0.5], [0.6, -1], [0.7, -2]])\n",
    "\n",
    "# manually set biases as well\n",
    "net.biases[0] = np.array([[1.0], [1.0]])\n",
    "net.biases[1] = np.array([[2.0], [2.0], [2.0]])\n",
    "\n",
    "print(\"\\nnew weights:\")\n",
    "printValues(net.weights)\n",
    "\n",
    "print(\"new biases: (list of \" + str(type(net.biases[0])) + \")\")\n",
    "print(type(net.biases[0]))\n",
    "printValues(net.biases)\n",
    "\n",
    "mine = mynet.Network(sizes)\n",
    "mine.weights = copy.deepcopy(net.weights)\n",
    "mine.biases = copy.deepcopy(net.biases)\n",
    "mine.sizes = copy.deepcopy(net.sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate output for given input\n",
    "# returns list of the activation at each layer\n",
    "def evaluate(net, cur):\n",
    "    #print(\"expected output = \" + str(net.feedforward(cur))) # known solution\n",
    "\n",
    "    # slow way of doing it:\n",
    "#     for layer in range(net.num_layers-1):\n",
    "#         tmp = []\n",
    "#         for node in range(len(net.weights[layer])):\n",
    "#             w = net.weights[layer][node]\n",
    "#             b = net.biases[layer][node]\n",
    "#             print(\"w = \" + str(w))\n",
    "#             print(\"b = \" + str(b))\n",
    "#             x = np.dot(w, cur) + b\n",
    "#             x = network.sigmoid(x)\n",
    "#             tmp.append([x[0]])\n",
    "#         cur = tmp\n",
    "#         print(\"new cur = \" + str(cur) + \"\\n\")\n",
    "\n",
    "    activations = []\n",
    "    #print(\"\\n\\nfaster way:\")\n",
    "    # fast way of doing it:\n",
    "    a = cur # input\n",
    "    activations.append(a)\n",
    "    for b,w in zip(net.biases, net.weights):\n",
    "        a = network.sigmoid(np.dot(w,a)+b)\n",
    "        activations.append(a)\n",
    "        #print(str(b) + \"\\n\" + str(w) + \"\\n -> \" + str(a) + \"\\n\")\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do backpropogation\n",
    "# (currently just calculates errors at each node)\n",
    "def backprop(cur, expected):\n",
    "    activations = evaluate(net, cur)\n",
    "    print(\"\\nactivations:\")\n",
    "    print(activations)\n",
    "    \n",
    "    nabla_b = [np.zeros(b.shape, dtype=float) for b in net.biases]\n",
    "    nabla_w = [np.zeros(w.shape, dtype=float) for w in net.weights]\n",
    "\n",
    "    \n",
    "    # array to store the calculated error at each node as we backpropogate\n",
    "    # errors[l][j] = dC/dzl_0\n",
    "    errors = []\n",
    "    for n in net.sizes:\n",
    "        errors.append(np.zeros((n, 1), dtype=float))\n",
    "    #print('errors:')\n",
    "    #print(errors)\n",
    "    # traverse layers backwards\n",
    "    # TODO: consider adding a dummy layer of to the beginning of the weights and biases vectors\n",
    "    #       so the indicies make sense and match up with the errors array, etc\n",
    "    for l in range(len(net.sizes)-1, 0, -1):\n",
    "        index = l-1 # notes: subtract 1 from layer number to index into weights and biases arrays\n",
    "        #print(\"at layer \" + str(l))\n",
    "        # traverse nodes in layer l\n",
    "        for j in range(net.sizes[l]):\n",
    "            al_j = activations[l][j]\n",
    "            # if we're on the last layer\n",
    "            if l == len(net.sizes)-1:\n",
    "                #errors[l][j] = 2*(al_j-expected[j]) * al_j*(1-al_j) # BP1\n",
    "                errors[l][j] = (al_j-expected[j]) * al_j*(1-al_j) # BP1\n",
    "            else:\n",
    "                # BP2\n",
    "                total = 0\n",
    "                for k in range(0, net.sizes[l+1]):\n",
    "                    #print(\"k = \" + str(k) + \", weight -> \" + str(net.weights[index+1][k][j]))\n",
    "                    #print(\"error -> \" + str(errors[l+1][k][0]))\n",
    "                    total += net.weights[index+1][k][j] * errors[l+1][k][0]\n",
    "                errors[l][j] = al_j*(1-al_j) + total\n",
    "\n",
    "            # TODO: as test update the bias and weights corresponding to the error just calculated\n",
    "            # (not sure if doing this right)\n",
    "            #net.biases[index][j] += -0.25 * errors[l][j]\n",
    "            nabla_b[index][j] = errors[l][j]\n",
    "            if (l != 0):\n",
    "                for k in range(0, net.sizes[l-1]):\n",
    "                    nabla_w[index][j] = errors[l][j]*activations[l-1][k]\n",
    "                    #net.weights[index][j] += -0.25 * errors[l][j]*activations[l-1][k]\n",
    "    \n",
    "    return (nabla_b, nabla_w)\n",
    "    #errors = np.zeros(tuple(net.sizes), dtype=float)\n",
    "    #print(\"final errors:\")\n",
    "    #print(errors) # (TODO: hand check these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their activations=\n",
      "[array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4]]), array([[0.98201379],\n",
      "       [0.99999917]]), array([[0.87984967],\n",
      "       [0.8305048 ],\n",
      "       [0.66539081]])]\n",
      "nabla_b:\n",
      "[array([[ 2.40024389e-03],\n",
      "       [-2.65207413e-07]]), array([[ 0.09301263],\n",
      "       [-0.02385926],\n",
      "       [ 0.14814652]])]\n",
      "nabla_w:\n",
      "[array([[ 2.40024389e-03,  4.80048779e-03,  7.20073168e-03,\n",
      "         9.60097557e-03],\n",
      "       [-2.65207413e-07, -5.30414826e-07, -7.95622239e-07,\n",
      "        -1.06082965e-06]]), array([[ 0.09133969,  0.09301255],\n",
      "       [-0.02343012, -0.02385924],\n",
      "       [ 0.14548193,  0.1481464 ]])]\n",
      "----\n",
      "\n",
      "activations:!!\n",
      "[array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4]]), array([[0.98201379],\n",
      "       [0.99999917]]), array([[0.87984967],\n",
      "       [0.8305048 ],\n",
      "       [0.66539081]])]\n",
      "nabla_b:\n",
      "[array([[ 2.40024389e-03],\n",
      "       [-2.65207413e-07]]), array([[ 0.09301263],\n",
      "       [-0.02385926],\n",
      "       [ 0.14814652]])]\n",
      "nabla_w:\n",
      "[array([[ 2.40024389e-03,  4.80048779e-03,  7.20073168e-03,\n",
      "         9.60097557e-03],\n",
      "       [-2.65207413e-07, -5.30414826e-07, -7.95622239e-07,\n",
      "        -1.06082965e-06]]), array([[ 0.09133969,  0.09301255],\n",
      "       [-0.02343012, -0.02385924],\n",
      "       [ 0.14548193,  0.1481464 ]])]\n"
     ]
    }
   ],
   "source": [
    "# test backpropogation to see if we can get it to work for 2 or 3 training examples\n",
    "\n",
    "data = np.array([[1], [2], [3], [4]]) # input\n",
    "expected = np.array([[0.0], [1.0], [0.0]]) # known output for this input\n",
    "\n",
    "# other training data\n",
    "data2 = np.array([[-1], [-4], [-99], [0.3]])\n",
    "expected2 = np.array([[0.333], [0.333], [0.333]])\n",
    "\n",
    "data3 = np.array([[7.0], [4], [9], [6]])\n",
    "expected3 = np.array([[0], [0], [1.0]])\n",
    "\n",
    "nabla_b, nabla_w = net.backprop(data, expected)\n",
    "print(\"nabla_b:\")\n",
    "print(nabla_b)\n",
    "print(\"nabla_w:\")\n",
    "print(nabla_w)\n",
    "\n",
    "print(\"----\")\n",
    "mine_b, mine_w = mine.backprop(data, expected)\n",
    "print(\"nabla_b:\")\n",
    "print(mine_b)\n",
    "print(\"nabla_w:\")\n",
    "print(mine_w)\n",
    "\n",
    "# for i in range(4000):\n",
    "#     if i%3 ==0:\n",
    "#         backprop(data, expected)\n",
    "#     elif i%3 ==1:\n",
    "#         #backprop(data2, expected2)\n",
    "#         pass\n",
    "#     else:\n",
    "#         backprop(data3, expected3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
